{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srijayashree/CS598/blob/main/Model_finetuning_2tests.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "O9JQP5swbYMC"
      },
      "outputs": [],
      "source": [
        "import torch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llZxeI5_blg7",
        "outputId": "d2ccc358-4e84-467c-c269-f7c7536ef155"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.28.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n"
          ]
        }
      ],
      "source": [
        "pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Wojb2mQb-xs",
        "outputId": "8ba20fa1-5461-4661-982c-9efd8b711376"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertTokenizer, BertForMaskedLM, BertForSequenceClassification\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext')\n",
        "model = BertForMaskedLM.from_pretrained('microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "CZww-7zocAPx"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "serlX_ybrELZ"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/patient_wICD_shortconcat_4000.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "2bY0bzQkyDEL"
      },
      "outputs": [],
      "source": [
        "my_list = df['SHORT_CONCAT'].tolist()\n",
        "#tokenized_text = df.apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\n",
        "tokenized_list = [tokenizer.encode(text, add_special_tokens=True) for text in my_list]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fyyHUQpC4MF0",
        "outputId": "a449b089-5923-46ba-f60a-5ffae599045b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max length: 86\n"
          ]
        }
      ],
      "source": [
        "# Find the maximum length of the tokenized sequences\n",
        "max_len = max(len(tokenized_seq) for tokenized_seq in tokenized_list)\n",
        "\n",
        "print(\"Max length:\", max_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "QD9t8bH-z_aS"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, tokenized_text):\n",
        "        self.tokenized_text = tokenized_text\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tokenized_text)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.tokenized_text[idx])\n",
        "\n",
        "# pad the tokenized sequences\n",
        "padded_text = pad_sequence([torch.tensor(x[:max_len]) for x in tokenized_list], \n",
        "                           batch_first=True, padding_value=0)\n",
        "\n",
        "#padded_tensor = torch.tensor(padded_text)\n",
        "\n",
        "# dataset = TextDataset(tokenized_list)\n",
        "# create the dataset from the padded sequences\n",
        "dataset = torch.utils.data.TensorDataset(padded_text)\n",
        "\n",
        "# create the DataLoader object with batch size and shuffle settings\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "7oJv35__0IIY"
      },
      "outputs": [],
      "source": [
        "from transformers import AdamW\n",
        "\n",
        "#optimizer = AdamW(model.parameters(), lr=4e-5)\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=4e-5)\n",
        "\n",
        "model.train()\n",
        "\n",
        "for epoch in range(1):\n",
        "    for batch in dataloader:\n",
        "        # batch = torch.tensor(batch)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch[0], labels=batch[0])\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "kbEIyZapGxVV"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained('fine_tuned_model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "mgo4luimTVcW"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "# Define the decoder\n",
        "hidden_size = model.config.hidden_size\n",
        "hidden_size\n",
        "num_labels = 2 # heart failure & diabetes\n",
        "decoder = nn.Linear(hidden_size, num_labels)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dOvlDfykK45P",
        "outputId": "6a75162d-e042-4377-ff52-8bd421e4f489"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "768"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "D1c_gtPPUoPc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "\n",
        "class sEHR_CE(nn.Module):\n",
        "    def __init__(self, num_classes=2, learning_rate=1e-5):\n",
        "        super(sEHR_CE, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.learning_rate = learning_rate\n",
        "        self.tokenizer = BertTokenizer.from_pretrained('microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext')\n",
        "        self.sehr = BertForSequenceClassification.from_pretrained('fine_tuned_model', output_hidden_states=True)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.linear = nn.Linear(self.sehr.config.hidden_size, num_classes)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        output = self.sehr(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        #print(output)\n",
        "        last_hidden_state = output.hidden_states[-1]  # get the last hidden state\n",
        "        cls_hidden_state = last_hidden_state[:, 0, :]\n",
        "        dropout = self.dropout(cls_hidden_state)\n",
        "        linear = self.linear(dropout)\n",
        "        return self.sigmoid(linear)\n",
        "\n",
        "    def train_model(self, train_dataloader, val_dataloader, num_epochs=3):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
        "        total_steps = len(train_dataloader) * num_epochs\n",
        "        scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=self.learning_rate,\n",
        "                                                  total_steps=total_steps)\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            self.train()\n",
        "            train_losses = []\n",
        "            for step, batch in enumerate(train_dataloader):\n",
        "                input_ids = batch['input_ids']\n",
        "                attention_mask = batch['attention_mask']\n",
        "                #labels = batch['labels']\n",
        "                labels = torch.stack([torch.tensor(batch['HF'], dtype=torch.float),\n",
        "                                      torch.tensor(batch['Diabetes'], dtype=torch.float)], dim=1)\n",
        "\n",
        "                outputs = self(input_ids=input_ids, attention_mask=attention_mask)\n",
        "                loss = nn.BCELoss()(outputs, labels)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "\n",
        "                train_losses.append(loss.item())\n",
        "\n",
        "            train_loss = sum(train_losses) / len(train_losses)\n",
        "\n",
        "            self.eval()\n",
        "            val_losses = []\n",
        "            val_accuracies = []\n",
        "            with torch.no_grad():\n",
        "                for batch in val_dataloader:\n",
        "                    input_ids = batch['input_ids']\n",
        "                    attention_mask = batch['attention_mask']\n",
        "                    #labels = batch['labels']\n",
        "                    labels = torch.stack([torch.tensor(batch['HF'], dtype=torch.float),\n",
        "                                      torch.tensor(batch['Diabetes'], dtype=torch.float)], dim=1)\n",
        "\n",
        "                    outputs = self(input_ids=input_ids, attention_mask=attention_mask)\n",
        "                    loss = nn.BCELoss()(outputs, labels)\n",
        "\n",
        "                    val_losses.append(loss.item())\n",
        "\n",
        "                    predicted_labels = (outputs > 0.5).float()\n",
        "                    accuracy = (predicted_labels == labels).float().mean()\n",
        "                    val_accuracies.append(accuracy.item())\n",
        "\n",
        "            val_loss = sum(val_losses) / len(val_losses)\n",
        "            val_accuracy = sum(val_accuracies) / len(val_accuracies)\n",
        "\n",
        "            print(f\"Epoch {epoch+1}/{num_epochs} - \"\n",
        "                  f\"Train loss: {train_loss:.4f} - \"\n",
        "                  f\"Val loss: {val_loss:.4f} - \"\n",
        "                  f\"Val accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "    def eval_model(self, test_dataloader):\n",
        "        self.eval()\n",
        "        # test_losses = []\n",
        "        # test_accuracies = []\n",
        "        # self.model.eval()\n",
        "        test_loss = 0\n",
        "        total_preds = []\n",
        "        total_labels = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in test_dataloader:\n",
        "                input_ids = batch['input_ids']\n",
        "                attention_mask = batch['attention_mask']\n",
        "                #labels = batch['labels']\n",
        "                labels = torch.stack([torch.tensor(batch['HF'], dtype=torch.float),\n",
        "                                      torch.tensor(batch['Diabetes'], dtype=torch.float)], dim=1)\n",
        "\n",
        "                outputs = self(input_ids=input_ids,attention_mask=attention_mask)\n",
        "                                 #labels=labels)\n",
        "                loss = nn.BCELoss()(outputs, labels)\n",
        "            \n",
        "                #loss = outputs.loss\n",
        "                #logits = outputs.logits\n",
        "                test_loss += loss.item()\n",
        "\n",
        "                #preds = torch.sigmoid(logits)\n",
        "                #preds = outputs.numpy()\n",
        "                #labels = labels.numpy()\n",
        "\n",
        "                total_preds.append(outputs)\n",
        "                total_labels.append(labels)\n",
        "\n",
        "        avg_test_loss = test_loss / len(test_dataloader)\n",
        "        total_preds = np.concatenate(total_preds, axis=0)\n",
        "        total_labels = np.concatenate(total_labels, axis=0)\n",
        "\n",
        "        return avg_test_loss, total_preds, total_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "-oyZvTA8ez3c"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, RandomSampler\n",
        "\n",
        "class TextDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data, tokenizer):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.data['SHORT_CONCAT'][idx]\n",
        "        #labels = [self.data['HF'][idx], self.data['Diabetes'][idx]]\n",
        "        HF = self.data['HF'][idx]\n",
        "        Diabetes = self.data['Diabetes'][idx]\n",
        "        \n",
        "        # tokenize the text\n",
        "        encoded_text = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=128,\n",
        "            pad_to_max_length=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        input_ids = encoded_text['input_ids'][0]\n",
        "        attention_mask = encoded_text['attention_mask'][0]\n",
        "        #return {'input_ids': input_ids, 'attention_mask': attention_mask, 'labels': labels}\n",
        "        return {'input_ids': input_ids, 'attention_mask': attention_mask, 'HF': HF, 'Diabetes': Diabetes}\n",
        "\n",
        "# load the data from the CSV files\n",
        "train_df = pd.read_csv('patient_split_shortmask_1_6000.csv')\n",
        "valid_df = pd.read_csv('patient_split_shortmask_4_4000.csv')\n",
        "test_df = pd.read_csv('patient_split_shortmask_3_4000.csv')\n",
        "\n",
        "# create the tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext')\n",
        "\n",
        "# create the datasets\n",
        "train_dataset = TextDataset(train_df, tokenizer)\n",
        "valid_dataset = TextDataset(valid_df, tokenizer)\n",
        "test_dataset = TextDataset(test_df, tokenizer)\n",
        "\n",
        "# create the dataloaders\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=32, shuffle=False)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "j5WNhrTvNDs3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 920
        },
        "id": "9OfiQL8yh3Im",
        "outputId": "83aa5fdc-5fd2-428e-d14e-d581caac19cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at fine_tuned_model were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at fine_tuned_model and are newly initialized: ['classifier.bias', 'classifier.weight', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "<ipython-input-15-a19f26677f99>:40: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.stack([torch.tensor(batch['HF'], dtype=torch.float),\n",
            "<ipython-input-15-a19f26677f99>:41: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  torch.tensor(batch['Diabetes'], dtype=torch.float)], dim=1)\n",
            "<ipython-input-15-a19f26677f99>:63: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.stack([torch.tensor(batch['HF'], dtype=torch.float),\n",
            "<ipython-input-15-a19f26677f99>:64: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  torch.tensor(batch['Diabetes'], dtype=torch.float)], dim=1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2 - Train loss: 0.4499 - Val loss: 0.2283 - Val accuracy: 0.9107\n",
            "Epoch 2/2 - Train loss: 0.2370 - Val loss: 0.2139 - Val accuracy: 0.9083\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-a19f26677f99>:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.stack([torch.tensor(batch['HF'], dtype=torch.float),\n",
            "<ipython-input-15-a19f26677f99>:98: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  torch.tensor(batch['Diabetes'], dtype=torch.float)], dim=1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recall at 0.5: [0.4478595  0.96480583]\n",
            "AUC: [0.84760337 0.99617169]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "<ipython-input-15-a19f26677f99>:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.stack([torch.tensor(batch['HF'], dtype=torch.float),\n",
            "<ipython-input-15-a19f26677f99>:98: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  torch.tensor(batch['Diabetes'], dtype=torch.float)], dim=1)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-f5828a7e6327>\u001b[0m in \u001b[0;36m<cell line: 39>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m# evaluate the model on the test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mavg_test_loss2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_preds2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_labels2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataloader2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;31m# calculate recall at 0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-a19f26677f99>\u001b[0m in \u001b[0;36meval_model\u001b[0;34m(self, test_dataloader)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_dataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m                 \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m                 \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    632\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-2014f8f6e0da>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# tokenize the text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         encoded_text = self.tokenizer.encode_plus(\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mencode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2715\u001b[0m         )\n\u001b[1;32m   2716\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2717\u001b[0;31m         return self._encode_plus(\n\u001b[0m\u001b[1;32m   2718\u001b[0m             \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2719\u001b[0m             \u001b[0mtext_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m_encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    647\u001b[0m             )\n\u001b[1;32m    648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 649\u001b[0;31m         \u001b[0mfirst_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    650\u001b[0m         \u001b[0msecond_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtext_pair\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36mget_input_ids\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    633\u001b[0m                     )\n\u001b[1;32m    634\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m                     raise ValueError(\n\u001b[0m\u001b[1;32m    636\u001b[0m                         \u001b[0;34mf\"Input {text} is not valid. Should be a string, a list/tuple of strings or a list/tuple of\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m                         \u001b[0;34m\" integers.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input nan is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers."
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import precision_score, recall_score, roc_auc_score, precision_recall_curve, auc\n",
        "# number of epochs to train the model\n",
        "n_epochs = 2\n",
        "\n",
        "# create an instance of the sEHR_CE class\n",
        "model1 = sEHR_CE()\n",
        "\n",
        "# train the model\n",
        "model1.train_model(train_dataloader, valid_dataloader, n_epochs)\n",
        "\n",
        "# evaluate the model on the test set\n",
        "avg_test_loss, total_preds, total_labels = model1.eval_model(test_dataloader)\n",
        "\n",
        "# calculate recall at 0.5\n",
        "pred_labels = (total_preds > 0.5).astype(int)\n",
        "recall = recall_score(total_labels, pred_labels, average=None)\n",
        "\n",
        "# calculate AUC\n",
        "auc = roc_auc_score(total_labels, total_preds, average=None)\n",
        "\n",
        "print(\"Recall at 0.5:\", recall)\n",
        "print(\"AUC:\", auc)\n",
        "\n",
        "total_labels_df = pd.DataFrame(total_labels)\n",
        "total_labels_df.to_csv('/content/total_labels.csv', index=False)\n",
        "\n",
        "total_preds_df = pd.DataFrame(total_preds)\n",
        "total_preds_df.to_csv('/content/total_preds_v2.csv', index=False)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 2nd set of tests with the disease terms removed\n",
        "test_df2 = pd.read_csv('patient_split_shortremoved_3_4000.csv')\n",
        "test_dataset2 = TextDataset(test_df2, tokenizer)\n",
        "test_dataloader2 = DataLoader(test_dataset2, batch_size=32, shuffle=False)\n",
        "\n",
        "# evaluate the model on the test set\n",
        "avg_test_loss2, total_preds2, total_labels2 = model1.eval_model(test_dataloader2)\n",
        "\n",
        "# calculate recall at 0.5\n",
        "pred_labels2 = (total_preds2 > 0.5).astype(int)\n",
        "recall2 = recall_score(total_labels2, pred_labels2, average=None)\n",
        "\n",
        "# calculate AUC\n",
        "auc2 = roc_auc_score(total_labels2, total_preds2, average=None)\n",
        "\n",
        "print(\"Recall2 at 0.5:\", recall2)\n",
        "print(\"AUC2:\", auc2)\n",
        "\n",
        "total_labels_df2 = pd.DataFrame(total_labels2)\n",
        "total_labels_df2.to_csv('/content/total_labels_removed.csv', index=False)\n",
        "\n",
        "total_preds_df2 = pd.DataFrame(total_preds2)\n",
        "total_preds_df2.to_csv('/content/total_preds_removed.csv', index=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer"
      ],
      "metadata": {
        "id": "F8adA5l5MjfT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97e15b21-66cf-4383-d774-d91011b077fb"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertTokenizer(name_or_path='microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext', vocab_size=30522, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dk94cYgQiRXn",
        "outputId": "5822e8a4-82bc-40b1-eec2-b055f7cb0b74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recall at 0.5: [0.4478595  0.96480583]\n",
            "AUC: [0.84760337 0.99617169]\n",
            "Avg test loss: 0.21502231754129753\n"
          ]
        }
      ],
      "source": [
        "precision = precision_score(total_labels, pred_labels, average=None)\n",
        "recall = recall_score(total_labels, pred_labels, average=None)\n",
        "\n",
        "# calculate AUC\n",
        "auc = roc_auc_score(total_labels, total_preds, average=None)\n",
        "\n",
        "print(\"Recall at 0.5:\", recall)\n",
        "print(\"AUC:\", auc)\n",
        "print(\"Avg test loss:\", avg_test_loss)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(total_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTQfngcyIPej",
        "outputId": "b492081f-1f10-45ba-f415-9e23a8cf257d"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0.]\n",
            " [0. 0.]\n",
            " [0. 0.]\n",
            " ...\n",
            " [0. 1.]\n",
            " [0. 0.]\n",
            " [0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_labels_df = pd.DataFrame(total_labels)\n",
        "total_labels_df.to_csv('/content/total_labels.csv', index=False)"
      ],
      "metadata": {
        "id": "ioFmOZBEMp7K"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhASR3Y1tE0I"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NCnqt5ac70b",
        "outputId": "1233d58a-7aa6-4e06-f46d-b54dccecc4ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.1365695  0.00739421]\n",
            " [0.21475925 0.01261922]\n",
            " [0.21051845 0.00665803]\n",
            " ...\n",
            " [0.30202112 0.9932414 ]\n",
            " [0.03938677 0.01561123]\n",
            " [0.1079238  0.00450369]]\n"
          ]
        }
      ],
      "source": [
        "print(total_preds)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_preds_df = pd.DataFrame(total_preds)\n",
        "total_preds_df.to_csv('/content/total_preds_v2.csv', index=False)\n"
      ],
      "metadata": {
        "id": "FpuK3ZypIemT"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2nd set of tests with the disease terms removed\n",
        "test_df2 = pd.read_csv('patient_split_shortremoved_3_4000.csv')\n",
        "test_dataset2 = TextDataset(test_df2, tokenizer)\n",
        "test_dataloader2 = DataLoader(test_dataset2, batch_size=32, shuffle=False)\n",
        "\n",
        "# evaluate the model on the test set\n",
        "avg_test_loss2, total_preds2, total_labels2 = model1.eval_model(test_dataloader2)\n",
        "\n",
        "# calculate recall at 0.5\n",
        "pred_labels2 = (total_preds2 > 0.5).astype(int)\n",
        "recall2 = recall_score(total_labels2, pred_labels2, average=None)\n",
        "\n",
        "# calculate AUC\n",
        "auc2 = roc_auc_score(total_labels2, total_preds2, average=None)\n",
        "\n",
        "print(\"Recall2 at 0.5:\", recall2)\n",
        "print(\"AUC2:\", auc2)\n",
        "\n",
        "total_labels_df2 = pd.DataFrame(total_labels2)\n",
        "total_labels_df2.to_csv('/content/total_labels_removed.csv', index=False)\n",
        "\n",
        "total_preds_df2 = pd.DataFrame(total_preds2)\n",
        "total_preds_df2.to_csv('/content/total_preds_removed.csv', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lD9afARpmTWh",
        "outputId": "1b46324c-3ffb-4d19-ec11-3f5ac993a9e9"
      },
      "execution_count": 41,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "<ipython-input-15-a19f26677f99>:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.stack([torch.tensor(batch['HF'], dtype=torch.float),\n",
            "<ipython-input-15-a19f26677f99>:98: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  torch.tensor(batch['Diabetes'], dtype=torch.float)], dim=1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recall2 at 0.5: [0.54945055 0.00121951]\n",
            "AUC2: [0.84552347 0.53127876]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyMxeqfR9ElRLW9oX2FoW6bI",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}